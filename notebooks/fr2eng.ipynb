{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b402d31a",
   "metadata": {},
   "source": [
    "# French to English Translation with PyTorch\n",
    "\n",
    "Building a sequence-to-sequence model from scratch using pure PyTorch.\n",
    "\n",
    "This notebook demonstrates the complete pipeline:\n",
    "1. Data acquisition and preprocessing\n",
    "2. Vocabulary building  \n",
    "3. Seq2Seq model architecture\n",
    "4. Training and evaluation\n",
    "\n",
    "All data is stored locally in `./data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49350fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random \n",
    "import re\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using device:{device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58210e04",
   "metadata": {},
   "source": [
    "## Step 1: Acquisition\n",
    "\n",
    "Download and prepare data the giga-fren dataset(French - English parallel corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3d734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data: /home/hayk.minasyan/Project/translation/data\n",
      "   Raw: /home/hayk.minasyan/Project/translation/data/raw\n",
      "   Processed: /home/hayk.minasyan/Project/translation/data/processed\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('./data')\n",
    "raw_path = data_path/'raw'\n",
    "processed_path = data_path/'processed'\n",
    "\n",
    "raw_path.mkdir(exist_ok=True)\n",
    "processed_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"   Data: {data_path.absolute()}\")\n",
    "print(f\"   Raw: {raw_path.absolute()}\")\n",
    "print(f\"   Processed: {processed_path.absolute()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05312167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset already exists in data/giga-fren.tgz\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "dataset_url = \"https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz\"\n",
    "\n",
    "dataset_file = data_path/\"giga-fren.tgz\"\n",
    "\n",
    "if not dataset_file.exists():\n",
    "    print(f\"Downloading {dataset_url} ...\")\n",
    "    print(\"This will take a few minutes(2,4 GB) ... \")\n",
    "    !wget {dataset_url} -O {dataset_file}\n",
    "    print(f\"Downloaded to {dataset_file}\")\n",
    "else:\n",
    "    print(f\"âœ… Dataset already exists in {dataset_file}\")\n",
    "\n",
    "# Extract the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6a9ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted\n",
      "\n",
      "Files extracted:\n",
      "  giga-fren.release2.fixed.en: 3.53 GB\n",
      "  giga-fren.release2.fixed.fr: 4.25 GB\n"
     ]
    }
   ],
   "source": [
    "import tarfile \n",
    "\n",
    "en_file = raw_path/'giga-fren'/'giga-fren.release2.fixed.en'\n",
    "fr_file = raw_path/'giga-fren'/'giga-fren.release2.fixed.fr'\n",
    "\n",
    "if not en_file.exists() or not fr_file.exists():\n",
    "    print(\"Extracting dataset...\")\n",
    "    with tarfile.open(dataset_file, 'r:gz') as tar:\n",
    "        tar.extractall(path = raw_path)\n",
    "    print(f\"Extracted to {raw_path}\")\n",
    "else:\n",
    "    print(\"Dataset already extracted\")\n",
    "\n",
    "\n",
    "print(f\"\\nFiles extracted:\")\n",
    "for f in (raw_path/'giga-fren').glob('*.fixed.*'):\n",
    "    size_gb = f.stat().st_size / (1024**3)\n",
    "    print(f\"  {f.name}: {size_gb:.2f} GB\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d808fd4",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploration\n",
    "\n",
    "Examine the raw dataset (22 million sentence pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8616c3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statisitcs\n",
      "Enlglish sentences: 22,520,376\n",
      "French sentences: 22,520,376\n"
     ]
    }
   ],
   "source": [
    "en_file = raw_path/'giga-fren'/'giga-fren.release2.fixed.en'\n",
    "fr_file = raw_path/'giga-fren'/'giga-fren.release2.fixed.fr'\n",
    "\n",
    "with open(en_file, 'r', encoding = 'utf-8') as f:\n",
    "    en_count = sum(1 for _ in f)\n",
    "with open(fr_file, 'r', encoding = 'utf-8') as f:\n",
    "    fr_count = sum(1 for _ in f)\n",
    "\n",
    "print(\"Dataset Statisitcs\")\n",
    "print(f\"Enlglish sentences: {en_count:,}\")\n",
    "print(f\"French sentences: {fr_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d7d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Sample sentences:\n",
      "\n",
      "1.\n",
      "  EN: Changing Lives | Changing Society | How It Works | Technology Drives Change Home | Concepts | Teachers | Search | Overview | Credits | HHCC Web | Reference | Feedback Virtual Museum of Canada Home Page\n",
      "  FR: Il a transformÃ© notre vie | Il a transformÃ© la sociÃ©tÃ© | Son fonctionnement | La technologie, moteur du changement Accueil | Concepts | Enseignants | Recherche | AperÃ§u | Collaborateurs | Web HHCC | Ressources | Commentaires MusÃ©e virtuel du Canada\n",
      "\n",
      "2.\n",
      "  EN: Site map\n",
      "  FR: Plan du site\n",
      "\n",
      "3.\n",
      "  EN: Feedback\n",
      "  FR: RÃ©troaction\n",
      "\n",
      "4.\n",
      "  EN: Credits\n",
      "  FR: CrÃ©dits\n",
      "\n",
      "5.\n",
      "  EN: FranÃ§ais\n",
      "  FR: English\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ“ Sample sentences:\")\n",
    "with open(en_file, 'r', encoding='utf-8') as en, \\\n",
    "     open(fr_file, 'r', encoding='utf-8') as fr:\n",
    "    for i, (en_line, fr_line) in enumerate(zip(en, fr)):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        print(f\"\\n{i+1}.\")\n",
    "        print(f\"  EN: {en_line.strip()}\")\n",
    "        print(f\"  FR: {fr_line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22392ab",
   "metadata": {},
   "source": [
    "## Step 3: Data filtering\n",
    "\n",
    "The full dataset is too large and contains mixed content. I'll extract only questions for a focused dataset.\n",
    "\n",
    "- English: Questions starting with \"Wh\" (What, Where, When, Why, Which, Who)\n",
    "- French: Sentences ending with \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aceb16ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering questions from 22M sentences ... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 1,000,000 | Found: 3,481 questions\n",
      "Processed: 2,000,000 | Found: 7,078 questions\n",
      "Processed: 3,000,000 | Found: 9,095 questions\n",
      "Processed: 4,000,000 | Found: 13,058 questions\n",
      "Processed: 5,000,000 | Found: 16,997 questions\n",
      "Processed: 6,000,000 | Found: 20,941 questions\n",
      "Processed: 7,000,000 | Found: 22,480 questions\n",
      "Processed: 8,000,000 | Found: 24,955 questions\n",
      "Processed: 9,000,000 | Found: 27,302 questions\n",
      "Processed: 10,000,000 | Found: 29,759 questions\n",
      "Processed: 11,000,000 | Found: 32,019 questions\n",
      "Processed: 12,000,000 | Found: 34,163 questions\n",
      "Processed: 13,000,000 | Found: 36,305 questions\n",
      "Processed: 14,000,000 | Found: 38,272 questions\n",
      "Processed: 15,000,000 | Found: 40,282 questions\n",
      "Processed: 16,000,000 | Found: 42,189 questions\n",
      "Processed: 17,000,000 | Found: 44,179 questions\n",
      "Processed: 18,000,000 | Found: 45,644 questions\n",
      "Processed: 19,000,000 | Found: 46,975 questions\n",
      "Processed: 20,000,000 | Found: 48,493 questions\n",
      "Processed: 21,000,000 | Found: 49,841 questions\n",
      "Processed: 22,000,000 | Found: 51,556 questions\n",
      "Filtering complete!\n",
      "Total processed: 22,520,376\n",
      "Questions found: 52331\n",
      "   Reduction: 430.3x smaller\n"
     ]
    }
   ],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_fq = re.compile('^([^?.!]+\\?)') \n",
    "\n",
    "print(\"Filtering questions from 22M sentences ... \")\n",
    "\n",
    "questions = []\n",
    "count = 0\n",
    "\n",
    "with open(en_file, 'r', encoding = 'utf-8') as en_f, \\\n",
    "    open(fr_file, 'r', encoding = 'utf-8') as fr_f:\n",
    "\n",
    "    for en_line, fr_line in zip(en_f, fr_f):\n",
    "        count += 1\n",
    "\n",
    "        en_match = re_eq.search(en_line)\n",
    "        fr_match = re_fq.search(fr_line)\n",
    "\n",
    "        if en_match and fr_match:\n",
    "            questions.append({\n",
    "                'en': en_match.group().lower(),\n",
    "                'fr': fr_match.group().lower()\n",
    "            })\n",
    "\n",
    "        if count % 1_000_000== 0:\n",
    "            print(f\"Processed: {count:,} | Found: {len(questions):,} questions\")\n",
    "\n",
    "print(f\"Filtering complete!\")\n",
    "print(f\"Total processed: {count:,}\")\n",
    "print(f\"Questions found: {len(questions)}\")\n",
    "print(f\"   Reduction: {count/len(questions):.1f}x smaller\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d317e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 52,331questions to:\n",
      "Saved 52,331 questions to:\n",
      "   /home/hayk.minasyan/Project/translation/data/questions.csv\n",
      "\n",
      "ðŸ“Š Dataset preview:\n",
      "                                                  en  \\\n",
      "0                                    what is light ?   \n",
      "1                                        who are we?   \n",
      "2                            where did we come from?   \n",
      "3                       what would we do without it?   \n",
      "4  what is the absolute location (latitude and lo...   \n",
      "5  what is the major aboriginal group on vancouve...   \n",
      "6  what are the advantages and disadvantages of u...   \n",
      "7  what types of land cover are associated with t...   \n",
      "8                  what is the population of canada?   \n",
      "9              which province is the most populated?   \n",
      "\n",
      "                                                  fr  \n",
      "0                          quâ€™est-ce que la lumiÃ¨re?  \n",
      "1                                    oÃ¹ sommes-nous?  \n",
      "2                                  d'oÃ¹ venons-nous?  \n",
      "3                       que ferions-nous sans elle ?  \n",
      "4  quelle sont les coordonnÃ©es (latitude et longi...  \n",
      "5  quel est le groupe autochtone principal sur lâ€™...  \n",
      "6  quels sont les avantages et les dÃ©savantages d...  \n",
      "7  Ã  quel type de couverture des terres associez-...  \n",
      "8               quelle est la population du canada ?  \n",
      "9           quelle est la province la plus peuplÃ©e ?  \n",
      "\n",
      "ðŸ“ˆ Statistics:\n",
      "   French avg length: 13.1 words\n",
      "   English avg length: 11.6 words\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(questions)\n",
    "\n",
    "csv_path = data_path/ 'questions.csv'\n",
    "df.to_csv(csv_path, index = False)\n",
    "\n",
    "print(F\"Saved {len(df):,}questions to:\")\n",
    "print(f\"Saved {len(df):,} questions to:\")\n",
    "print(f\"   {csv_path.absolute()}\")\n",
    "print(f\"\\nðŸ“Š Dataset preview:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nðŸ“ˆ Statistics:\")\n",
    "print(f\"   French avg length: {df['fr'].str.split().str.len().mean():.1f} words\")\n",
    "print(f\"   English avg length: {df['en'].str.split().str.len().mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e972478",
   "metadata": {},
   "source": [
    "## Step 4: Vocabulary Building\n",
    "\n",
    "Create word-to-index mappings for French and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c93a2a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:\n",
      "   French: 10004\n",
      "   English: 10004\n",
      "Special tokens (indices):\n",
      "   <pad>: 0\n",
      "   <unk>: 1\n",
      "   <sos>: 2\n",
      "   <eos>: 3\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences, max_size = 10000, min_freq = 2):\n",
    "    counter = Counter()\n",
    "    for sent in sentences:\n",
    "        counter.update(sent.split())\n",
    "\n",
    "    vocab = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "\n",
    "    vocab += [word for word, count in counter.most_common(max_size)\n",
    "    if count >= min_freq]\n",
    "\n",
    "    #create mappings\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx:word for idx, word in enumerate(vocab)}\n",
    "\n",
    "    return vocab, word2idx, idx2word\n",
    "\n",
    "print(\"Building vocabulary ...\")\n",
    "fr_vocab, fr_word2idx, fr_idx2word = build_vocab(df['fr'].tolist())\n",
    "en_vocab, en_word2idx, en_idx2word = build_vocab(df['en'].tolist())\n",
    "\n",
    "print(f\"Vocab size:\")\n",
    "print(f\"   French: {len(fr_vocab)}\")\n",
    "print(f\"   English: {len(en_vocab)}\")\n",
    "\n",
    "print(\"Special tokens (indices):\")\n",
    "print(f\"   <pad>: {fr_word2idx['<pad>']}\")\n",
    "print(f\"   <unk>: {fr_word2idx['<unk>']}\")\n",
    "print(f\"   <sos>: {fr_word2idx['<sos>']}\")\n",
    "print(f\"   <eos>: {fr_word2idx['<eos>']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d463b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies saved to data/processed/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "vocab_data = {\n",
    "    'fr_word2idx': fr_word2idx,\n",
    "    'en_word2idx': en_word2idx,\n",
    "    'fr_idx2word': fr_idx2word,\n",
    "    'en_idx2word': en_idx2word,\n",
    "    'fr_vocab': fr_vocab,\n",
    "    'en_vocab': en_vocab\n",
    "}\n",
    "\n",
    "vocab_path = processed_path/'vocab.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab_data, f)\n",
    "\n",
    "print(f\"Vocabularies saved to {vocab_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5625d48",
   "metadata": {},
   "source": [
    "## Load pre-trained fastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a7501ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_vec_file(fname, max_vectors=200000):\n",
    "    \"\"\"Load fastText .vec file\"\"\"\n",
    "    print(f\"Loading {fname}...\")\n",
    "    embeddings = {}\n",
    "    \n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        n, d = map(int, f.readline().split())\n",
    "        print(f\"  Total: {n:,} vectors, {d} dimensions\")\n",
    "        print(f\"  Loading first {max_vectors:,}...\")\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_vectors:\n",
    "                break\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "            \n",
    "            if (i + 1) % 50000 == 0:\n",
    "                print(f\"    Loaded {i+1:,}\")\n",
    "    \n",
    "    print(f\"  âœ… Loaded {len(embeddings):,} vectors\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41b84e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cc.fr.300.vec...\n",
      "  Total: 2,000,000 vectors, 300 dimensions\n",
      "  Loading first 200,000...\n",
      "    Loaded 50,000\n",
      "    Loaded 100,000\n",
      "    Loaded 150,000\n",
      "    Loaded 200,000\n",
      "  âœ… Loaded 200,000 vectors\n",
      "Loading cc.en.300.vec...\n",
      "  Total: 2,000,000 vectors, 300 dimensions\n",
      "  Loading first 200,000...\n",
      "    Loaded 50,000\n",
      "    Loaded 100,000\n",
      "    Loaded 150,000\n",
      "    Loaded 200,000\n",
      "  âœ… Loaded 200,000 vectors\n"
     ]
    }
   ],
   "source": [
    "project_path = Path('.')  # Current directory (translation folder)\n",
    "fr_vecs = load_vec_file(project_path / 'cc.fr.300.vec', max_vectors=200000)\n",
    "en_vecs = load_vec_file(project_path / 'cc.en.300.vec', max_vectors=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25a678a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding matrices...\n",
      "  Found 6125/10004 words (61.2%)\n",
      "  Found 6166/10004 words (61.6%)\n",
      "\n",
      "âœ… Embedding matrices created:\n",
      "   French: (10004, 300)\n",
      "   English: (10004, 300)\n",
      "âœ… Saved embeddings to data/processed/embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word2idx, vec_dict, emb_dim=300):\n",
    "    \"\"\"Create embedding matrix from pre-trained vectors\"\"\"\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_matrix = np.random.randn(vocab_size, emb_dim).astype('float32') * 0.01\n",
    "    \n",
    "    found = 0\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in vec_dict:\n",
    "            embedding_matrix[idx] = vec_dict[word]\n",
    "            found += 1\n",
    "    \n",
    "    print(f\"  Found {found}/{vocab_size} words ({found/vocab_size*100:.1f}%)\")\n",
    "    return embedding_matrix\n",
    "\n",
    "print(\"Creating embedding matrices...\")\n",
    "fr_embedding_matrix = create_embedding_matrix(fr_word2idx, fr_vecs)\n",
    "en_embedding_matrix = create_embedding_matrix(en_word2idx, en_vecs)\n",
    "\n",
    "print(f\"\\nâœ… Embedding matrices created:\")\n",
    "print(f\"   French: {fr_embedding_matrix.shape}\")\n",
    "print(f\"   English: {en_embedding_matrix.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "embeddings_data = {\n",
    "    'fr_embedding_matrix': fr_embedding_matrix,\n",
    "    'en_embedding_matrix': en_embedding_matrix\n",
    "    }\n",
    "\n",
    "emb_path = processed_path / 'embeddings.pkl'\n",
    "with open(emb_path, 'wb') as f:\n",
    "    pickle.dump(embeddings_data, f)\n",
    "\n",
    "print(f\"âœ… Saved embeddings to {emb_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
