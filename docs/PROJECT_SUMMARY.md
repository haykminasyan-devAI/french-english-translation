# French-to-English Neural Machine Translation
## Complete Project Summary

**Author**: Hayk Minasyan  
**Date**: January, 2026  
**Objective**: Compare three generations of neural machine translation architectures

---

## ğŸ¯ Project Overview

Built and trained three progressively advanced neural machine translation models on French-English parallel corpus, demonstrating evolution from basic RNNs to state-of-the-art Transformers.

---

## ğŸ“Š Final Results

| Model | Architecture | Parameters | Best Val Loss | BLEU Score | Improvement |
|-------|--------------|------------|---------------|------------|-------------|
| Model 1 | Bidirectional GRU Seq2Seq | 11.6M | 5.24 | **27.69** | Baseline |
| Model 2 | + Bahdanau Attention | 20.3M | 4.74 | **27.35** | Val: -9.5% |
| Model 3 | Transformer (Multi-head) | 65.8M | **4.03** â­ | **42.46** â­ | BLEU: +53% |

### **Winner: Model 3 (Transformer)**
- âœ… **Best BLEU Score**: 42.46 (vs 27.69/27.35)
- âœ… **Best Validation Loss**: 4.03  
- âœ… **Most Coherent Translations**
- âœ… **Fewer repetitive patterns**

---

## ğŸ“ Dataset

- **Source**: [Giga-Fren Corpus](https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz)
- **Original Size**: 22 million parallel sentences
- **Filtered**: 52,331 French-English question pairs
- **Split**: 80% train (41,864) / 10% val (5,233) / 10% test (5,234)
- **Pre-trained Embeddings**: fastText Common Crawl (300-dim)
  - French: 6,125/10,004 words (61.2% coverage)
  - English: 6,166/10,004 words (61.6% coverage)

---

## ğŸ¤– Model Architectures

### Model 1: Basic Seq2Seq (Baseline)
**Paper**: Sutskever et al. (2014)

**Architecture**:
- Encoder: 2-layer Bidirectional GRU (256 hidden units)
- Decoder: 2-layer Unidirectional GRU (256 hidden units)
- Embeddings: 300-dim (pre-trained fastText)

**Strengths**:
- Simple, fast training
- Good on short sentences

**Limitations**:
- Information bottleneck (fixed context vector)
- Many `<unk>` tokens
- Poor on long sentences

---

### Model 2: Seq2Seq + Bahdanau Attention
**Paper**: Bahdanau et al. (2015) - "Neural Machine Translation by Jointly Learning to Align and Translate"

**Architecture**:
- Encoder: Same as Model 1
- Decoder: 2-layer GRU with additive attention mechanism
- Embeddings: 300-dim (pre-trained fastText)

**Innovation**:
- Attention allows decoder to "focus" on relevant source words
- Dynamic context (not fixed-length)
- Interpretable attention weights

**Results**:
- 9.5% better validation loss than Model 1
- Similar BLEU (attention helps with loss calibration)
- Better on medium-length sentences

---

### Model 3: Transformer (State-of-the-Art)
**Paper**: Vaswani et al. (2017) - "Attention Is All You Need"

**Architecture**:
- Encoder: 6-layer Transformer (8-head self-attention, 512-dim, 2048 FFN)
- Decoder: 6-layer Transformer (masked self-attention + cross-attention)
- Positional Encoding: Sinusoidal
- Embeddings: 300-dim projected to 512-dim

**Innovations**:
- Multi-head self-attention (no recurrence!)
- Parallel computation (faster training)
- Layer normalization + residual connections
- Learning rate warmup (4000 steps)
- Label smoothing (0.1)

**Results**:
- **Best validation loss**: 4.03 (-23% vs Model 1)
- **Best BLEU**: 42.46 (+53% vs Models 1/2)
- Excellent on long sentences
- More fluent, coherent translations

---

## ğŸš€ Training Setup

**Hardware**:
- Cluster: SLURM with scalar6000q partition
- GPU: NVIDIA RTX A6000 (48 GB VRAM)
- CPU: 8 cores per GPU
- Memory: 32 GB RAM per GPU

**Software**:
- PyTorch 2.4.1 with CUDA 11.8
- Python 3.8
- Training time per model: 12-20 minutes

**Training Parameters**:
- Batch size: 128
- Epochs: 15
- Optimizer: Adam
- Gradient clipping: 1.0
- Dropout: 0.3 (Models 1-2), 0.1 (Model 3)

---

## ğŸ“ˆ Key Observations

### **Training Convergence**:

**Model 1**: Smooth, steady convergence  
- Epoch 1: Train 5.72, Val 5.75
- Epoch 15: Train 3.55, Val 5.29
- **Best at Epoch 14**: Val 5.24

**Model 2**: Lower final training loss  
- Epoch 1: Train 5.73, Val 5.76
- Epoch 15: Train 1.49, Val 5.23
- **Best at Epoch 7**: Val 4.74

**Model 3**: Warmup crucial, best final result  
- Epoch 1: Train 9.45, Val 9.45 (high initial loss)
- Epoch 15: Train 2.12, Val 4.22
- **Best at Epoch 11**: Val 4.03

---

## ğŸ”¬ Technical Insights

1. **Attention is crucial**: Model 2's attention dramatically improved validation loss

2. **Transformers excel**: Model 3 outperforms on both loss and BLEU despite slower initial convergence

3. **Pre-trained embeddings help**: 60% coverage provided strong foundation

4. **More parameters â‰  always better**: Model 2 (20M) had similar BLEU to Model 1 (11M), but Model 3 (66M) was significantly better

5. **Learning rate matters**: Transformer's warmup schedule was essential for stability

---

## ğŸ“ Sample Translation Comparison

### **Short & Simple** (all models succeed):
```
FR: quoi de neuf ?
Model 1: what's new?  âœ…
Model 2: what's new?  âœ…
Model 3: what's new?  âœ…
```

### **Medium Complexity** (Transformer best):
```
FR: quelles sont les principales Ã©tapes du processus de nÃ©gociation?
TRUE:    what are the key steps in the negotiation process?
Model 1: what are the main steps of the <unk>
Model 2: what are the key steps in the process process process the process process?
Model 3: what are the main steps of the negotiating process?  â­ Best!
```

### **High Complexity** (Transformer significantly better):
```
FR: quelle est la position de l'union europÃ©enne sur le bien-Ãªtre des animaux?
TRUE:    what is the european union's position on animal welfare?
Model 1: what is the value of the <unk> of the <unk>
Model 2: what is the government's position on on the animal animal
Model 3: what is the european union position on animal animal <unk>  â­ Best!
```

---

## ğŸ“ What This Project Demonstrates

âœ… **Data Engineering**:
- Web-scraped data preprocessing
- Regex-based filtering
- Train/val/test splitting
- Vocabulary building

âœ… **NLP Techniques**:
- Word tokenization and indexing
- Pre-trained word embeddings integration
- Sequence-to-sequence learning
- Teacher forcing

âœ… **Deep Learning Architectures**:
- Recurrent Neural Networks (GRU)
- Attention Mechanisms (Bahdanau)
- Transformer (Multi-head Self-Attention)

âœ… **Engineering Best Practices**:
- GPU computing on HPC cluster
- SLURM job submission
- Model checkpointing
- Evaluation metrics (BLEU, perplexity)
- Visualization (loss curves, comparisons)

âœ… **Research Understanding**:
- Implementation of seminal papers
- Progression of NMT techniques
- Hyperparameter tuning
- Performance analysis

---

## ğŸ“š References

1. **Sutskever, I., Vinyals, O., & Le, Q. V.** (2014). Sequence to Sequence Learning with Neural Networks. *NeurIPS 2014*.

2. **Bahdanau, D., Cho, K., & Bengio, Y.** (2015). Neural Machine Translation by Jointly Learning to Align and Translate. *ICLR 2015*.  
   https://arxiv.org/abs/1409.0473

3. **Vaswani, A., Shazeer, N., Parmar, N., et al.** (2017). Attention Is All You Need. *NeurIPS 2017*.  
   https://arxiv.org/abs/1706.03762

4. **Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T.** (2017). Enriching Word Vectors with Subword Information. *TACL 2017*.  
   https://arxiv.org/abs/1607.04606

---

## ğŸš€ Future Improvements

1. **Subword Tokenization**: Use BPE/SentencePiece to handle rare words
2. **Beam Search**: Replace greedy decoding for better translations
3. **Larger Dataset**: Use full 22M sentences (currently 52K)
4. **Model Ensemble**: Combine predictions from all 3 models
5. **Fine-tuning**: More epochs, hyperparameter search
6. **Attention Visualization**: Plot attention weights to interpret model behavior

---

## ğŸ“ Project Structure

```
translation/
â”œâ”€â”€ fr2eng.ipynb                    # Data preparation notebook
â”œâ”€â”€ train__model_1.py               # Model 1: Basic Seq2Seq
â”œâ”€â”€ train__model_2.py               # Model 2: + Bahdanau Attention  
â”œâ”€â”€ train__model_3.py               # Model 3: Transformer
â”œâ”€â”€ run_training_model*.sh          # SLURM job scripts
â”œâ”€â”€ evaluate_model*.py              # Evaluation scripts
â”œâ”€â”€ visualize_results.py            # Training curve visualizations
â”œâ”€â”€ best_model*.pt                  # Trained model checkpoints
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original corpus
â”‚   â””â”€â”€ processed/                  # Filtered questions + vocabularies
â”œâ”€â”€ logs/                           # Training logs
â”œâ”€â”€ *.png                           # Visualization plots
â”œâ”€â”€ MODELS_SUMMARY.md               # Technical documentation
â”œâ”€â”€ FINAL_RESULTS.md                # Results summary
â””â”€â”€ PROJECT_SUMMARY.md              # This file
```

---

## ğŸ¯ Conclusion

Successfully implemented three generations of neural machine translation models, achieving:

- âœ… **42.46 BLEU** on test set (Transformer)
- âœ… **53% improvement** over baseline
- âœ… **Complete understanding** of NMT evolution
- âœ… **Production-ready** GPU training pipeline

The Transformer architecture (Model 3) clearly demonstrates the power of self-attention mechanisms over traditional RNN-based approaches, validating the "Attention Is All You Need" principle.

---

**Total Training Time**: ~40 minutes (all 3 models on GPU)  
**Hardware Used**: NVIDIA RTX A6000  
**Final Deliverables**: 3 trained models, evaluation metrics, visualizations, documentation
