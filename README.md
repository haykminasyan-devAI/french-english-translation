# French-to-English Neural Machine Translation

A comprehensive implementation of three generations of neural machine translation architectures, from basic RNNs to state-of-the-art Transformers.

## ğŸ¯ Project Overview

This project implements and compares three progressively advanced neural machine translation models trained on the Giga-Fren French-English parallel corpus.

## ğŸ“Š Results

| Model | Architecture | BLEU Score | Val Loss | Parameters |
|-------|--------------|------------|----------|------------|
| Model 1 | Bidirectional GRU Seq2Seq | 27.69 | 5.24 | 11.6M |
| Model 2 | + Bahdanau Attention | 27.35 | 4.74 | 20.3M |
| Model 3 | Transformer (Multi-head) | **42.46** â­ | **4.03** â­ | 65.8M |

**Best Model**: Model 3 achieves 42.46 BLEU score with 53% improvement over baseline.

## ğŸš€ Quick Start

### Prerequisites
```bash
Python 3.8+
PyTorch 2.4+ with CUDA support
Virtual environment (venv)
```

### Installation

```bash
# Clone/navigate to project
cd translation

# Create virtual environment
python3 -m virtualenv venv
source venv/bin/activate

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install pandas numpy matplotlib tqdm jupyter
```

### Train Models

```bash
# On SLURM cluster with GPU:
sbatch scripts/slurm/run_training_model1.sh
sbatch scripts/slurm/run_training_model2.sh
sbatch scripts/slurm/run_training_model3.sh

# On local machine:
python scripts/train/train__model_1.py
```

### Evaluate Models

```bash
# Evaluate specific model
python scripts/evaluate/evaluate_model3.py

# Compare all models
python scripts/evaluate/evaluate_all_models.py

# Interactive translator
python scripts/evaluate/interactive_translator.py
```

### Visualize Results

```bash
python scripts/evaluate/visualize_results.py
# Generates plots in visualizations/
```

## ğŸ“ Project Structure

```
translation/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Original Giga-Fren corpus (22M sentences)
â”‚   â”œâ”€â”€ processed/               # Filtered questions (52K pairs)
â”‚   â”‚   â”œâ”€â”€ questions.csv
â”‚   â”‚   â”œâ”€â”€ vocab.pkl
â”‚   â”‚   â””â”€â”€ embeddings.pkl
â”‚   â”œâ”€â”€ giga-fren.tgz           # Downloaded dataset
â”‚   â””â”€â”€ questions.csv           # Processed data
â”œâ”€â”€ models/                      # Trained model checkpoints
â”‚   â”œâ”€â”€ model1/
â”‚   â”‚   â”œâ”€â”€ best_model1.pt
â”‚   â”‚   â””â”€â”€ model1_test_results.pkl
â”‚   â”œâ”€â”€ model2/
â”‚   â”‚   â””â”€â”€ best_model2.pt
â”‚   â””â”€â”€ model3/
â”‚       â””â”€â”€ best_model3.pt
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train/                   # Training scripts
â”‚   â”‚   â”œâ”€â”€ train__model_1.py
â”‚   â”‚   â”œâ”€â”€ train__model_2.py
â”‚   â”‚   â””â”€â”€ train__model_3.py
â”‚   â”œâ”€â”€ evaluate/                # Evaluation scripts
â”‚   â”‚   â”œâ”€â”€ evaluate_model1.py
â”‚   â”‚   â”œâ”€â”€ evaluate_model2.py
â”‚   â”‚   â”œâ”€â”€ evaluate_model3.py
â”‚   â”‚   â”œâ”€â”€ evaluate_all_models.py
â”‚   â”‚   â”œâ”€â”€ evaluate_with_metrics.py
â”‚   â”‚   â”œâ”€â”€ interactive_translator.py
â”‚   â”‚   â”œâ”€â”€ show_translations.py
â”‚   â”‚   â”œâ”€â”€ compare_all_translations.py
â”‚   â”‚   â””â”€â”€ visualize_results.py
â”‚   â””â”€â”€ slurm/                   # SLURM job submission scripts
â”‚       â”œâ”€â”€ run_training_model1.sh
â”‚       â”œâ”€â”€ run_training_model2.sh
â”‚       â””â”€â”€ run_training_model3.sh
â”œâ”€â”€ logs/                        # Training logs
â”‚   â”œâ”€â”€ model1_*.log
â”‚   â”œâ”€â”€ model2_*.log
â”‚   â””â”€â”€ model3_*.log
â”œâ”€â”€ visualizations/              # Generated plots
â”‚   â”œâ”€â”€ training_comparison.png
â”‚   â”œâ”€â”€ Model_1_Basic_Seq2Seq_training.png
â”‚   â”œâ”€â”€ Model_2_+_Attention_training.png
â”‚   â””â”€â”€ Model_3_Transformer_training.png
â”œâ”€â”€ notebooks/                   # Jupyter notebooks
â”‚   â””â”€â”€ fr2eng.ipynb
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ MODELS_SUMMARY.md
â”‚   â”œâ”€â”€ FINAL_RESULTS.md
â”‚   â””â”€â”€ PROJECT_SUMMARY.md
â”œâ”€â”€ cc.fr.300.vec               # Pre-trained embeddings
â”œâ”€â”€ cc.en.300.vec
â””â”€â”€ venv/                       # Virtual environment
```

## ğŸ“– Documentation

- **[MODELS_SUMMARY.md](docs/MODELS_SUMMARY.md)**: Technical specifications of all models
- **[FINAL_RESULTS.md](docs/FINAL_RESULTS.md)**: Detailed results and analysis
- **[PROJECT_SUMMARY.md](docs/PROJECT_SUMMARY.md)**: Complete project overview

## ğŸ¤– Models

### Model 1: Basic Seq2Seq
- **Architecture**: Bidirectional GRU encoder + GRU decoder
- **Reference**: Sutskever et al. (2014)
- **Training**: `scripts/train/train__model_1.py`

### Model 2: Seq2Seq with Attention
- **Architecture**: GRU + Bahdanau additive attention
- **Reference**: Bahdanau et al. (2015)
- **Training**: `scripts/train/train__model_2.py`

### Model 3: Transformer
- **Architecture**: Multi-head self-attention (6 layers, 8 heads)
- **Reference**: Vaswani et al. (2017) "Attention Is All You Need"
- **Training**: `scripts/train/train__model_3.py`

## ğŸ“š References

1. Bahdanau et al. (2015): https://arxiv.org/abs/1409.0473
2. Vaswani et al. (2017): https://arxiv.org/abs/1706.03762
3. FastText embeddings: https://fasttext.cc/

## ğŸ“ Author

Hayk Minasyan  
January 2026

## ğŸ“ License

Educational/Research purposes
